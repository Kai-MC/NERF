{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dSJWVIQMyJaj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzRt7tElyJal"
      },
      "source": [
        "# Part 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9Lhk9PXyJam"
      },
      "outputs": [],
      "source": [
        "class SinusoidalPositionalEncoding(nn.Module):\n",
        "    def __init__(self, L=10):\n",
        "        super(SinusoidalPositionalEncoding, self).__init__()\n",
        "        self.L = L\n",
        "\n",
        "    def forward(self, x):\n",
        "        pe = [x]\n",
        "        for i in range(self.L):\n",
        "            for fn in [torch.sin, torch.cos]:\n",
        "                pe.append(fn(2.0 ** i * x))\n",
        "        return torch.cat(pe, dim=-1)\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, hidden_dim=256, num_layers=3):\n",
        "        super(MLP, self).__init__()\n",
        "        layers = [nn.Linear(in_dim, hidden_dim), nn.ReLU()]\n",
        "        for _ in range(num_layers - 2):\n",
        "            layers.extend([nn.Linear(hidden_dim, hidden_dim), nn.ReLU()])\n",
        "        layers.append(nn.Linear(hidden_dim, out_dim))\n",
        "        layers.append(nn.Sigmoid())\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "class NeuralField(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, hidden_dim=256, num_layers=3, L=10):\n",
        "        super(NeuralField, self).__init__()\n",
        "        self.pe = SinusoidalPositionalEncoding(L)\n",
        "        self.mlp = MLP(in_dim * (2 * L + 1), out_dim, hidden_dim, num_layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pe(x)\n",
        "        return self.mlp(x)\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, image_path, N):\n",
        "        self.image = Image.open(image_path).convert('RGB')\n",
        "        self.transform = transforms.ToTensor()\n",
        "        self.N = N\n",
        "        self.w, self.h = self.image.size\n",
        "\n",
        "        # Generate random coordinates\n",
        "        x_coords = np.random.randint(0, self.w, N)\n",
        "        y_coords = np.random.randint(0, self.h, N)\n",
        "        self.coordinates = torch.tensor(np.stack((x_coords / self.w, y_coords / self.h), axis=1), dtype=torch.float32)\n",
        "\n",
        "        # Transform the image to a tensor and extract colors\n",
        "        image_tensor = self.transform(self.image)\n",
        "        self.colors = image_tensor[:, y_coords, x_coords].permute(1, 0).to(torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.N\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.coordinates, self.colors[idx]\n",
        "\n",
        "    def get_all_data(self):\n",
        "        return self.coordinates, self.colors\n",
        "\n",
        "def psnr(mse):\n",
        "    return 10 * math.log10(1 / mse)\n",
        "\n",
        "def generate_image(model, image_path, device):\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    transform = transforms.ToTensor()\n",
        "    tensor = transform(image).to(device).float()\n",
        "    h, w = tensor.shape[1], tensor.shape[2]\n",
        "    coords = np.array([[x / w, y / h] for y in range(h) for x in range(w)])\n",
        "    coords = torch.from_numpy(coords).to(device).float()\n",
        "    colors = model(coords)\n",
        "    colors = colors.view(h, w, 3).cpu().detach().numpy()\n",
        "    return colors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "po0ZsHvlyJam",
        "outputId": "162c8967-1f9d-4379-f3bf-fd12cd1bd5f1"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = NeuralField(2, 3).to(device)\n",
        "images = []\n",
        "optimizer = optim.Adam(model.parameters(), lr=2e-3)\n",
        "criterion = nn.MSELoss()\n",
        "losses = []\n",
        "\n",
        "image_path = './panda.jpg'\n",
        "\n",
        "image = generate_image(model, image_path, device)\n",
        "images.append(image)\n",
        "\n",
        "for iteration in range(1, 2001):\n",
        "    dataset = ImageDataset(image_path, N=10000)\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    coords, colors = dataset.get_all_data()\n",
        "    coords, colors = coords.to(device), colors.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(coords)\n",
        "    loss = criterion(outputs, colors)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    running_loss += loss.item()\n",
        "\n",
        "    loss = running_loss / len(coords)\n",
        "    losses.append(loss)\n",
        "\n",
        "    if (iteration)% 250 == 0:\n",
        "        print(f'Iteration {iteration}/2000, Loss: {loss}')\n",
        "        image = generate_image(model, image_path, device)\n",
        "        images.append(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPsTFAZAyJan",
        "outputId": "eb25b81f-8622-4c2d-e2ec-fbf6cf31d74a"
      },
      "outputs": [],
      "source": [
        "num_rows = 3\n",
        "num_cols = 3\n",
        "fig, axes = plt.subplots(num_rows, num_cols, figsize=(10, 10))  # Adjust the size as needed\n",
        "\n",
        "for i, img in enumerate(images):\n",
        "    row = i // num_cols\n",
        "    col = i % num_cols\n",
        "    axes[row, col].imshow(img)\n",
        "    axes[row, col].axis('off')  # Turn off axis numbers and labels\n",
        "    axes[row, col].set_title(f\"Iteration {0+250*i}\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZyPo4NVeyJan",
        "outputId": "aae61029-12d2-48c0-bb2a-b03f5b0da15e"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'losses' is a list of loss values, one for each iteration\n",
        "iterations = range(1, 2001)  # 3000 iterations, starting from 1\n",
        "psnr_values = [psnr(mse) if mse != 0 else float('inf') for mse in losses]\n",
        "\n",
        "plt.figure(figsize=(10, 6))  # Adjust the size as needed\n",
        "plt.plot(iterations, psnr_values, label='Loss per Iteration')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('PSNR')\n",
        "plt.title('PSNR During Training')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utqiikrvyJan"
      },
      "source": [
        "# Part 2.2: Sampling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGrNfm_OyJan"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "data = np.load(f\"lego_200x200.npz\")\n",
        "\n",
        "# Training images: [100, 200, 200, 3]\n",
        "images_train = data[\"images_train\"] / 255.0\n",
        "\n",
        "# Cameras for the training images\n",
        "# (camera-to-world transformation matrix): [100, 4, 4]\n",
        "c2ws_train = data[\"c2ws_train\"]\n",
        "\n",
        "# Validation images:\n",
        "images_val = data[\"images_val\"] / 255.0\n",
        "\n",
        "# Cameras for the validation images: [10, 4, 4]\n",
        "# (camera-to-world transformation matrix): [10, 200, 200, 3]\n",
        "c2ws_val = data[\"c2ws_val\"]\n",
        "\n",
        "# Test cameras for novel-view video rendering:\n",
        "# (camera-to-world transformation matrix): [60, 4, 4]\n",
        "c2ws_test = data[\"c2ws_test\"]\n",
        "\n",
        "# Camera focal length\n",
        "focal = data[\"focal\"]  # float"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vhm-a6GsyJan"
      },
      "outputs": [],
      "source": [
        "def transform(c2w, X_c):\n",
        "    '''\n",
        "    Transforms points from camera coordinates (X_c) to world coordinates using PyTorch.\n",
        "    X_c is a batch of 3-element vectors.\n",
        "    '''\n",
        "    # Add a column of ones to X_c to convert to homogeneous coordinates\n",
        "    ones = torch.ones(X_c.shape[0], 1, device=X_c.device)\n",
        "    X_c_homogeneous = torch.cat([X_c, ones], dim=1)\n",
        "\n",
        "    # Transform using the c2w matrix\n",
        "    X_w_homogeneous = torch.matmul(X_c_homogeneous, c2w.T)\n",
        "\n",
        "    # Extract the first three elements (x, y, z coordinates) from the result\n",
        "    X_w = X_w_homogeneous[:, :3]\n",
        "\n",
        "    return X_w\n",
        "\n",
        "def pixel_to_camera(K, uv, s):\n",
        "\n",
        "    # Invert the intrinsic matrix\n",
        "    K_inv = torch.inverse(K)\n",
        "\n",
        "    # Add one to uv and mutiply by s\n",
        "    ones = torch.ones(uv.shape[0], 1)\n",
        "    uv_homog = torch.cat([uv, ones], dim=1) * s\n",
        "\n",
        "    # Transform to camera coordinates\n",
        "    x_c = torch.mm(uv_homog, K_inv.T)\n",
        "\n",
        "    return x_c\n",
        "\n",
        "def pixel_to_ray(K, c2w, uv, s=1):\n",
        "\n",
        "    # Calculate ray direction\n",
        "    x_c = pixel_to_camera(K, uv, s)\n",
        "    # Transform camera coordinates to world coordinates\n",
        "    x_w = transform(c2w, x_c)\n",
        "\n",
        "\n",
        "    w2c = torch.inverse(c2w)\n",
        "    R = w2c[:3, :3]\n",
        "    t = w2c[:3, 3]\n",
        "\n",
        "    ray_o = -torch.matmul(torch.inverse(R), t.unsqueeze(1)).squeeze(-1)\n",
        "    ray_o = ray_o.expand_as(x_w)\n",
        "    ray_d = torch.nn.functional.normalize(x_w - ray_o, p=2, dim=1)\n",
        "\n",
        "    return ray_o, ray_d\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwmbYePmyJao"
      },
      "outputs": [],
      "source": [
        "class RaysData:\n",
        "    def __init__(self, images, K, c2ws):\n",
        "        \"\"\"\n",
        "        Initialize the RaysData object.\n",
        "        \"\"\"\n",
        "        self.images = images\n",
        "        self.K = K\n",
        "        self.c2ws = c2ws\n",
        "        self.H, self.W = images[0].shape[:2]\n",
        "\n",
        "        self.uvs = self.generate_uvs()\n",
        "        self.pixels = self.generate_pixels()\n",
        "\n",
        "\n",
        "    def sample_rays(self, N, M):\n",
        "        \"\"\"\n",
        "        Samples rays from the images.\n",
        "\n",
        "        :param N: Total number of rays to sample.\n",
        "        :param M: Number of images to sample from.\n",
        "        :return: Ray origins, ray directions, and pixel colors.\n",
        "        \"\"\"\n",
        "        num_images = self.images.shape[0]\n",
        "\n",
        "\n",
        "        # Sample M images\n",
        "        selected_indices = np.random.choice(num_images, M, replace=False)\n",
        "        selected_images = self.images[selected_indices]\n",
        "        selected_c2ws = self.c2ws[selected_indices]\n",
        "\n",
        "        # Calculate number of rays per image\n",
        "        rays_per_image = N // M\n",
        "\n",
        "        # Initialize lists to store ray origins, directions, and colors\n",
        "        ray_origins = []\n",
        "        ray_directions = []\n",
        "        pixel_colors = []\n",
        "\n",
        "        for i in range(M):\n",
        "            image = selected_images[i]\n",
        "            c2w = selected_c2ws[i]\n",
        "            c2w = torch.tensor(c2w, dtype=torch.float32)\n",
        "\n",
        "            height, width, _ = image.shape\n",
        "\n",
        "            uv = np.column_stack((\n",
        "                np.random.randint(0, width, rays_per_image) + 0.5,\n",
        "                np.random.randint(0, height, rays_per_image) + 0.5\n",
        "            ))\n",
        "\n",
        "            # Convert to tensor\n",
        "            uv = torch.tensor(uv, dtype=torch.float32)\n",
        "\n",
        "            # Get ray origins and directions\n",
        "            r_o, r_d = pixel_to_ray(self.K, c2w, uv)\n",
        "\n",
        "            # Store the results\n",
        "            ray_origins.append(r_o)\n",
        "            ray_directions.append(r_d)\n",
        "\n",
        "            # Retrieve and store pixel colors\n",
        "            uv_pixel = uv.long()\n",
        "\n",
        "            colors = image[uv_pixel[:, 1], uv_pixel[:, 0]]\n",
        "            colors = torch.tensor(colors, dtype=torch.float32)\n",
        "            pixel_colors.append(colors)\n",
        "\n",
        "        # Concatenate the results from all images\n",
        "        ray_origins = torch.cat(ray_origins, dim=0)\n",
        "        ray_directions = torch.cat(ray_directions, dim=0)\n",
        "        pixel_colors = torch.cat(pixel_colors, dim=0)\n",
        "\n",
        "        return ray_origins, ray_directions, pixel_colors\n",
        "\n",
        "    def generate_uvs(self):\n",
        "        # Generate UV coordinates for each image\n",
        "        x = torch.arange(0, 200).tile(200)\n",
        "        y = torch.arange(0, 200).repeat_interleave(200)\n",
        "        combined_tensor = torch.stack((x, y), dim=1).repeat(100,1)\n",
        "        return combined_tensor\n",
        "\n",
        "    def generate_pixels(self):\n",
        "        # Generate pixel values for each image\n",
        "        images = torch.tensor(self.images, dtype=torch.float32)\n",
        "        color_grid = images.reshape(-1, 3)  # Reshape to [batch*height*width, channels]\n",
        "        return color_grid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Return all r_o, r_d, colors, return_uv of a single image\n",
        "# This is for generating the input of Val images\n",
        "class TestData:\n",
        "    def __init__(self, images, K, c2ws):\n",
        "        self.images = images\n",
        "        self.K = K\n",
        "        self.c2ws = c2ws\n",
        "        self.current_image_index = 0\n",
        "\n",
        "    def __iter__(self):\n",
        "        self.current_image_index = 0  \n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        if self.current_image_index < len(self.images):\n",
        "            # Get the image and camera-to-world matrix for the current index\n",
        "            image = self.images[self.current_image_index]\n",
        "            c2w = self.c2ws[self.current_image_index]\n",
        "\n",
        "            # Move c2w to the appropriate device\n",
        "            c2w = torch.tensor(c2w, dtype=torch.float32)\n",
        "\n",
        "            # Compute width and height\n",
        "            height, width, _ = image.shape\n",
        "\n",
        "            # Generate a grid of (u, v) pixel coordinates for the entire image\n",
        "            uv = np.mgrid[0:height, 0:width].reshape(2, -1).T\n",
        "            return_uv = uv.copy()\n",
        "            uv = uv + 0.5\n",
        "            uv = torch.from_numpy(uv).float()\n",
        "\n",
        "            # Get ray origins and directions\n",
        "            r_o, r_d = pixel_to_ray(self.K, c2w, uv)\n",
        "\n",
        "            \n",
        "            colors = image[return_uv[:, 1], return_uv[:, 0]]  \n",
        "            colors = torch.tensor(colors, dtype=torch.float32)\n",
        "\n",
        "            # Increment the index for the next image\n",
        "            self.current_image_index += 1\n",
        "\n",
        "            return r_o, r_d, colors, return_uv\n",
        "        else:\n",
        "            raise StopIteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CxT2BQOIyJao"
      },
      "outputs": [],
      "source": [
        "def sample_along_rays(ray_o, ray_d, near=2.0, far=6.0, n_samples=33, t_width=0.05, perturb=True):\n",
        "    # Uniformly sample t values\n",
        "    t = torch.linspace(near, far, n_samples, device=ray_o.device)\n",
        "    t = t.expand(ray_o.shape[0], n_samples).clone()\n",
        "\n",
        "    # Introduce perturbations during training\n",
        "    if perturb:\n",
        "        t += torch.rand(t.shape, device=ray_o.device) * t_width\n",
        "    # Calculate 3D coordinates for each sample along each ray\n",
        "    points_3d = ray_o.unsqueeze(1) + ray_d.unsqueeze(1) * t.unsqueeze(-1)\n",
        "\n",
        "    return points_3d[:,1:], (t[:, 1:] -t[:, :-1]).unsqueeze(-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJ6jHtPvyJao"
      },
      "source": [
        "## Define K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZ5xJQ1ryJao",
        "outputId": "42d7cd0a-f1ae-42f1-cea7-cd89ed4dfef5"
      },
      "outputs": [],
      "source": [
        "f_x = focal  # from the loaded data\n",
        "f_y = focal  # assuming square pixels\n",
        "image_width = 200  # example width\n",
        "image_height = 200  # example height\n",
        "o_x = image_width / 2\n",
        "o_y = image_height / 2\n",
        "\n",
        "# Intrinsic matrix K\n",
        "K = np.array([[f_x, 0, o_x],\n",
        "              [0, f_y, o_y],\n",
        "              [0, 0, 1]], dtype=np.float32)\n",
        "\n",
        "\n",
        "K = torch.from_numpy(K)\n",
        "\n",
        "# --- You Need to Implement These ------\n",
        "dataset = RaysData(images_train, K, c2ws_train)\n",
        "rays_o, rays_d, pixels = dataset.sample_rays(200, 100)\n",
        "points, t = sample_along_rays(rays_o, rays_d, perturb=True)\n",
        "\n",
        "print(rays_o.shape, rays_d.shape, pixels.shape, points.shape, dataset.uvs.shape, dataset.pixels.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lfmw-Tc8yJao"
      },
      "source": [
        "# Part 2.3: Putting the Dataloading All Together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i76IDTwXyJao"
      },
      "outputs": [],
      "source": [
        "import viser, time  # pip install viser\n",
        "import numpy as np\n",
        "\n",
        "# --- You Need to Implement These ------\n",
        "dataset = RaysData(images_train, K, c2ws_train)\n",
        "rays_o, rays_d, pixels = dataset.sample_rays(200,2)\n",
        "points, t = sample_along_rays(rays_o, rays_d, perturb=True)\n",
        "rays_o = rays_o.numpy()\n",
        "rays_d = rays_d.numpy()\n",
        "points = points.numpy()\n",
        "H, W = images_train.shape[1:3]\n",
        "# ---------------------------------------\n",
        "\n",
        "server = viser.ViserServer(share=True)\n",
        "for i, (image, c2w) in enumerate(zip(images_train, c2ws_train)):\n",
        "    server.add_camera_frustum(\n",
        "        f\"/cameras/{i}\",\n",
        "        fov=2 * np.arctan2(H / 2, K[0, 0]),\n",
        "        aspect=W / H,\n",
        "        scale=0.15,\n",
        "        wxyz=viser.transforms.SO3.from_matrix(c2w[:3, :3]).wxyz,\n",
        "        position=c2w[:3, 3],\n",
        "        image=image\n",
        "    )\n",
        "for i, (o, d) in enumerate(zip(rays_o, rays_d)):\n",
        "    server.add_spline_catmull_rom(\n",
        "        f\"/rays/{i}\", positions=np.stack((o, o + d * 6.0)),\n",
        "    )\n",
        "server.add_point_cloud(\n",
        "    f\"/samples\",\n",
        "    colors=np.zeros_like(points).reshape(-1, 3),\n",
        "    points=points.reshape(-1, 3),\n",
        "    point_size=0.02,\n",
        ")\n",
        "time.sleep(1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbs2U7FUyJap"
      },
      "source": [
        "# 2.4 Neural Radiance Field\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svvnfilpyJap"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def SinusoidalPositionalEncoding(x, L=4):\n",
        "    pe = [x]\n",
        "    for i in range(L):\n",
        "        for fn in [torch.sin, torch.cos]:\n",
        "            pe.append(fn(2.0 ** i * x))\n",
        "    return torch.cat(pe, dim=-1)\n",
        "\n",
        "\n",
        "# Define the neural network model as per the architecture provided in the image\n",
        "class NeRFMLP(nn.Module):\n",
        "    def __init__(self, L=4):\n",
        "        super(NeRFMLP, self).__init__()\n",
        "\n",
        "        pe_output_dim_x = 3 + L*6\n",
        "        pe_output_dim_rd = 3 + L*6\n",
        "\n",
        "        # Define the First part of MLP for the 'x' stream\n",
        "        self.mlp_first = nn.Sequential(\n",
        "            nn.Linear(pe_output_dim_x, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 256),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        # Define the Second part of MLP for the 'x' stream\n",
        "        self.mlp_second = nn.Sequential(\n",
        "            nn.Linear(256 + pe_output_dim_x, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 256)\n",
        "        )\n",
        "\n",
        "\n",
        "        # Define the fully connect layer for density estimation\n",
        "        self.fc_density_1 = nn.Linear(256,1)\n",
        "\n",
        "        # Define the fully connect layer for rgb\n",
        "        self.fc_rgb_1 = nn.Linear(256,256)\n",
        "        self.fc_rgb_2 = nn.Linear(256 + pe_output_dim_rd,128)\n",
        "        self.fc_rgb_3 = nn.Linear(128,3)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x, rd):\n",
        "        # Apply positional encoding\n",
        "        x_start = SinusoidalPositionalEncoding(x)\n",
        "        rd = SinusoidalPositionalEncoding(rd).unsqueeze(1).expand(-1, 32, -1)\n",
        "\n",
        "        # Pass through the first part of MLP for the 'x' stream\n",
        "        x = self.mlp_first(x_start)\n",
        "        # Inject the input (after PE) to the middle of MLP through concatenation\n",
        "        x = torch.cat((x, x_start), dim=-1)\n",
        "        # Pass through the second part of MLP for the 'x' stream\n",
        "        share =  self.mlp_second(x)\n",
        "\n",
        "\n",
        "        #### density layer ##############\n",
        "        density = self.fc_density_1(share)\n",
        "        density = F.relu(density)\n",
        "\n",
        "        #### rgb layer ##############\n",
        "\n",
        "        rgb = self.fc_rgb_1(share)\n",
        "        rgb = torch.cat((rgb,rd), dim = -1)\n",
        "        rgb = self.fc_rgb_2(rgb)\n",
        "        rgb = F.relu(rgb)\n",
        "        rgb = self.fc_rgb_3(rgb)\n",
        "        rgb = torch.sigmoid(rgb)\n",
        "\n",
        "        return rgb, density"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2.5 Color Rendering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8ravACNyJap"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def volrend(sigmas, rgbs, step_size):\n",
        "    \"\"\"\n",
        "    Compute the volume rendering equation for a batch of samples along a ray.\n",
        "    \"\"\"\n",
        "\n",
        "    alpha = 1 - torch.exp(-sigmas* step_size)\n",
        "    T_0 = torch.ones_like(alpha[:, 0, :]).unsqueeze(-1)\n",
        "    T_rest = torch.cumprod((1 - alpha), dim = 1)[:, :-1, :]\n",
        "    T = torch.concat((T_0,T_rest), dim=1)\n",
        "    C_r = torch.cumsum(T*alpha*rgbs, dim = 1)[:,-1,:]\n",
        "\n",
        "    return C_r\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ZRJOvwryJap"
      },
      "outputs": [],
      "source": [
        "# Preparation for visualize image 0\n",
        "\n",
        "f_x = focal  # from the loaded data\n",
        "f_y = focal  # assuming square pixels\n",
        "image_width = 200  # example width\n",
        "image_height = 200  # example height\n",
        "o_x = image_width / 2\n",
        "o_y = image_height / 2\n",
        "\n",
        "# Intrinsic matrix K\n",
        "K = np.array([[f_x, 0, o_x],\n",
        "              [0, f_y, o_y],\n",
        "              [0, 0, 1]], dtype=np.float32)\n",
        "\n",
        "K = torch.from_numpy(K)\n",
        "\n",
        "\n",
        "start_pt = 0\n",
        "end_pt = 40000\n",
        "# --- You Need to Implement These ------\n",
        "valset = RaysData(images_val, K, c2ws_val)\n",
        "uvs = valset.uvs[:end_pt]\n",
        "pixels_val = valset.pixels[:end_pt]\n",
        "uvs = uvs + 0.5\n",
        "c2w = torch.from_numpy(valset.c2ws[0]).float()\n",
        "rays_o_val, rays_d_val = pixel_to_ray(valset.K, c2w, uvs)\n",
        "points_val, t_val = sample_along_rays(rays_o_val, rays_d_val, perturb=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "l2v5vqFKYgx6",
        "outputId": "1ec8c812-b0d9-4ed6-fc37-298cb68046da"
      },
      "outputs": [],
      "source": [
        "plt.imshow(valset.images[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpTmqNmuyJap",
        "outputId": "0d0ab693-d7e8-4a3d-d755-9621ab0d7fbe"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = NeRFMLP()\n",
        "if torch.cuda.device_count() > 1:\n",
        "    model = nn.DataParallel(model)\n",
        "model = model.to(device)\n",
        "\n",
        "images = []\n",
        "optimizer = optim.Adam(model.parameters(), lr=5e-4)\n",
        "criterion = nn.MSELoss()\n",
        "losses = []\n",
        "val_losses = []\n",
        "dataset = RaysData(images_train, K, c2ws_train)\n",
        "H, W = images_train.shape[1:3]\n",
        "\n",
        "# image = generate_image(model, image_path, device)\n",
        "# images.append(image)\n",
        "\n",
        "for iteration in range(1, 3001):\n",
        "    # dataset = ImageDataset(image_path, N=10000)\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    rays_o, rays_d, pixels = dataset.sample_rays(10000, 100)\n",
        "    points, t= sample_along_rays(rays_o, rays_d, perturb=True)\n",
        "    rays_o, rays_d, pixels, points, t = rays_o.to(device), rays_d.to(device), pixels.to(device), points.to(device), t.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    # predict and render color\n",
        "    rgbs, density = model(points, rays_d)\n",
        "    color_pred = volrend(density, rgbs, t)\n",
        "\n",
        "    loss = criterion(color_pred, pixels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # loss = running_loss / pixels.shape[0]\n",
        "    losses.append(loss)\n",
        "\n",
        "    if (iteration)% 100 == 0:\n",
        "        with torch.no_grad():  # No gradient computation during evaluation\n",
        "            model.eval()\n",
        "            rays_d_val = rays_d_val.to(device)\n",
        "            points_val = points_val.to(device)\n",
        "            pixels_val = pixels_val.to(device)\n",
        "            t_val = t_val.to(device)\n",
        "            rgb, density = model(points_val, rays_d_val)\n",
        "            color_pred = volrend(density, rgb, t_val)\n",
        "            val_loss = criterion(color_pred, pixels_val).item()\n",
        "            val_losses.append(val_loss)\n",
        "            images.append(color_pred)\n",
        "            print(f'Iteration {iteration}/1000, Training Loss: {loss}, Val Loss: {val_loss}')\n",
        "            model.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "Hd8dSG0qyJap",
        "outputId": "1b7a20dc-5b27-4686-fc00-c42ceebea086"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def psnr(mse):\n",
        "    if mse == 0:\n",
        "        return float('inf')\n",
        "    return 10 * math.log10(1 / mse)\n",
        "\n",
        "iterations = range(1, 3001)  # 3000 iterations, starting from 1\n",
        "validation_iterations = range(100, 3001, 100)  # Validation points every 100 iterations\n",
        "\n",
        "psnr_values = [psnr(mse) for mse in losses]\n",
        "psnr_values_val = [psnr(mse) for mse in val_losses]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(iterations, psnr_values, label='Training PSNR')\n",
        "plt.plot(validation_iterations, psnr_values_val, label='Validation PSNR', marker='o')  # Adding a marker for clarity\n",
        "\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('PSNR')\n",
        "plt.title('PSNR During Training')\n",
        "plt.legend()\n",
        "plt.grid(True)  # Optional: Add grid for better readability\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 707
        },
        "id": "oA71t2tq2XyK",
        "outputId": "36858420-46df-4c49-e1f1-38ef19d7e08d"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_every_other_image(images, interval):\n",
        "    # Filter to get every fourth image from the list\n",
        "    selected_images = images[::interval]  # This selects every fourth image\n",
        "\n",
        "    # Calculate the number of images to plot\n",
        "    num_images = len(selected_images)\n",
        "\n",
        "    # Create subplots\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(15, 7))  # Adjust the size as needed\n",
        "\n",
        "    # Flatten the axes array for easy iteration\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    # Iterate over selected images and flattened axes to plot each image\n",
        "    for i, (ax, image) in enumerate(zip(axes, selected_images)):\n",
        "        # Reshape and transpose the image (adjust dimensions as needed)\n",
        "        ax.imshow(image.cpu().reshape(200, 200, 3))\n",
        "        ax.axis('off')  # Turn off axis\n",
        "\n",
        "        # Add a title for each image\n",
        "        iteration_number = (i+1) * 5 * 100  # Adjust the multiplier to match the interval\n",
        "        ax.set_title(f\"Iteration {iteration_number}\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_every_other_image(images, 5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HDIVCeULyJaq"
      },
      "outputs": [],
      "source": [
        "# Create an instance of the TestData class\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "rays_data_val = TestData(images_val, K, c2ws_val)\n",
        "\n",
        "model.eval()\n",
        "model.to(device)\n",
        "rendered_images = []\n",
        "for (rays_o, rays_d, pixels, uv) in rays_data_val:\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        \n",
        "        rays_o = rays_o.to(device)\n",
        "        rays_d = rays_d.to(device)\n",
        "        pixels = pixels.to(device)\n",
        "\n",
        "        # Sample points along rays and move to GPU\n",
        "        points_3d, t_values = sample_along_rays(rays_o, rays_d, perturb=False)\n",
        "        points_3d = points_3d.to(device)\n",
        "        t_values = t_values.to(device)\n",
        "\n",
        "        # Forward pass through the NeRF model to get the radiance field\n",
        "        rgbs, density = model(points_3d, rays_d)\n",
        "        \n",
        "        # Perform volume rendering\n",
        "        rendered_colors = volrend(density, rgbs, t_values)\n",
        "        rendered_images.append(rendered_colors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# first 5 val predicted images\n",
        "fig, axes = plt.subplots(1, 5, figsize=(20, 4))  \n",
        "\n",
        "# Plot first five images\n",
        "for i in range(5):\n",
        "    ax = axes[i]\n",
        "    ax.imshow(rendered_images[i+5].cpu().numpy().reshape(200,200,3).transpose(1,0,2))  \n",
        "    ax.set_title(f'Predicted Val image {(i+6)}')\n",
        "    ax.axis('off')  \n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Generate GIF using test c2w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F2UFFtmxdJ9f"
      },
      "outputs": [],
      "source": [
        "class GenerateData:\n",
        "    def __init__(self, K, c2ws):\n",
        "        self.K = K\n",
        "        self.c2ws = c2ws\n",
        "        self.current_pose_index = 0\n",
        "\n",
        "    def __iter__(self):\n",
        "        self.current_pose_index = 0\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        if self.current_pose_index < len(self.c2ws):\n",
        "            c2w = self.c2ws[self.current_pose_index]\n",
        "            c2w = torch.tensor(c2w, dtype=torch.float32)\n",
        "            \n",
        "            height, width = 200, 200  \n",
        "\n",
        "            # Generate a grid of (u, v) pixel coordinates for the entire image\n",
        "            uv = np.mgrid[0:height, 0:width].reshape(2, -1).T\n",
        "            uv = uv + 0.5\n",
        "            uv = torch.from_numpy(uv).float()\n",
        "\n",
        "            r_o, r_d = pixel_to_ray(self.K, c2w, uv)\n",
        "\n",
        "            self.current_pose_index += 1\n",
        "\n",
        "            return r_o, r_d, uv.long()\n",
        "        else:\n",
        "            raise StopIteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_rendered_images = []\n",
        "rays_data_test = GenerateData(K, c2ws_test)\n",
        "\n",
        "model.eval()\n",
        "rendered_images = []\n",
        "for (rays_o, rays_d, uv) in rays_data_test:\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        \n",
        "        rays_o = rays_o.to(device)\n",
        "        rays_d = rays_d.to(device)\n",
        "\n",
        "        points_3d, t_values = sample_along_rays(rays_o, rays_d, perturb=False)\n",
        "        points_3d = points_3d.to(device)\n",
        "        t_values = t_values.to(device)\n",
        "\n",
        "        # Forward pass through the NeRF model to get the radiance field\n",
        "        rgbs, density = model(points_3d, rays_d)\n",
        "        \n",
        "        # Perform volume rendering\n",
        "        rendered_colors = volrend(density, rgbs, t_values)\n",
        "        test_rendered_images.append(rendered_colors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 5, figsize=(20, 4))  \n",
        "\n",
        "# Plot first four images\n",
        "for i in range(5):\n",
        "    ax = axes[i]\n",
        "    ax.imshow(test_rendered_images[i].cpu().numpy().reshape(200,200,3).transpose(1,0,2))  \n",
        "    ax.set_title(f'Predicted Test image {(i+1)}')\n",
        "    ax.axis('off')  \n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save as a gif\n",
        "import imageio\n",
        "from PIL import Image\n",
        "\n",
        "output_gif_path = 'Rendered_Images_Animation.gif'\n",
        "\n",
        "# Create a list to hold the converted images\n",
        "images_for_gif = []\n",
        "\n",
        "for img in test_rendered_images:\n",
        "    # Ensure the image is in uint8\n",
        "    img = img.cpu().numpy().reshape(200,200,3).transpose(1,0,2)\n",
        "    if img.dtype != np.uint8:\n",
        "        img = (img * 255).astype(np.uint8)\n",
        "\n",
        "    # resize to (400,400)\n",
        "    img_pil = Image.fromarray(img)\n",
        "    img_resized = img_pil.resize((400, 400))\n",
        "    img_resized = np.array(img_resized)\n",
        "\n",
        "    \n",
        "    images_for_gif.append(img_resized )\n",
        "\n",
        "# Save the frames as an animated GIF\n",
        "imageio.mimsave(output_gif_path, images_for_gif, fps=10)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "BzRt7tElyJal"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
